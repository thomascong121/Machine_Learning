<html>
<head>
<title>COMP9417 19T2</title>
</head>
<body>
<h2 align=center>COMP9417 19T2 Assignment - project topics</h2>
<hr>

<!--
-->
<h3 align=center>Group Configuration</h3>
<p>
<b>NOTE:</b> this assignment is a team-based machine learning project.
The reason for this is to allow you to take on a reasonably challenging
amount of work, and to enhance your team skills.

<h4 align=center> Each team must be configured with 2-4 students currently
enrolled in the course.</h4>

<p> 
Teams can be made up of students from different tutorials.

<p>
However, if you prefer to work by yourself in a team of one person, for
whatever reason, please email course admins to obtain permission to do so
<b>before starting your assignment.</b>

<p> 
Here is the list of topics (more may become available, so check back from time
to time).

<hr>


<!----------------------------------------------------------------------------->

<h3 align=center>0: Self-proposed</h3>
The objective of this topic is to propose a machine learning problem,
source the dataset(s) and implement a method to solve it.
This will typically come from an area of work or research of
which you have some previous experience. 
<hr>

<!----------------------------------------------------------------------------->

<p>
<b>Topic 0: Propose your own topic</b>
<p>
<b>Task description</b><br>
Up to you. The basic conditions are:
<ul>
<li> it must involve some practical work with some implementation of machine
	learning
<li> you must send an email to one of the course coordinators
	with a description of what you are planning
	(a couple of paragraphs would usually be enough) that needs to be
	approved in an emailed reply before you start
<li> it must not involve double-dipping, i.e., be part of project for another
	course, or for research postgrads it must include a statement to the
	effect that it
        is not part of the main work planned for the thesis (although it can
	be related, e.g., if your research is on behavioural cloning, it could
	be on reinforcement learning, which is a related but different approach)
</ul>

<p>
<b>Team</b> 2 to 4 person team</b>
<p>
<b>Difficulty</b> assigned per topic (often 5/5).

<!----------------------------------------------------------------------------->

<hr>
<h3 align=center>1: Competitions & Challenges</h3>

The objective of these topics is to solve a machine learning competition or
challenge where the dataset(s) are provided and you must 
implement a method to solve problem.
<hr>

<!----------------------------------------------------------------------------->

<p>
<b>Topic 1.1: Machine learning and data mining competitions</b>
<p>
<b>Task description</b><br>
A number of sites now host regular competitions. You can enter a live
competition or work on the dataset from a past competition. The main site
hosting machine learning competitions is Kaggle, with the 
competitions hosted <a href="http://www.kaggle.com/competitions">here</a>.

<!-- L I V E  C O M P E T I T I O N S -->

<p>
<b>Note: only competitions in categories other than "Getting started" are
acceptable.</b>
However, you can select one from either Active or Completed competitions
to work on.
There are now several years worth of completed competitions that are still
useful to learn from.

<p>
Many research-oriented competitions are also hosted on Kaggle.
However, there are some long-running competitions that pre-date Kaggle.
For example, the ACM KDD Cup competitions have been running annually
since 1997.
These are challenging machine learning tasks, and are chosen to be
representative of problems faced in industry and academia. However, note
that in these competitions it is often the case that, as in many
real-world data mining applications, it is usually as important to get to
know the data, deal with issues in setting up the problem, etc. as it is
to work out how to actually do the learning.

A full list of KDD Cup competitions is available in the
<a href="http://www.kdd.org/kdd-cup">KDD Cup Archives</a>.

<p>
For any of the competitions, active or completed, 
the choice of competition is up to you, but you should select only
<b>one</b> of the competitions and work on that.

The basic conditions are:
<ul>
<li> assess carefully the time you will need to understand the competition
	requirements, get familiar with the data and run the algorithm(s)
	you plan to use
<li> you must send an email to one of the course admins with a description
	of what you are planning
        (a couple of paragraphs would usually be enough) that needs to be
        approved before you start
<li> your report and software must be submitted as for the other topics;
	for live competitions you can include your submission's placing on
	the leaderboard at submission time!
<!-- (but if you win the competition you can keep the prize money!) --> 
</ul>

<p>
You must follow the task description as closely as possible. In the
competitions that have closed, the test data are available, but you should
avoid testing against these until you are really sure you understand the
performance of your method(s). Test set performance should be evaluated
and appear in your report, as well as an indication of how your team's
performance matches those in the original competition (if available, but there
usually is a leaderboard or published papers by the top-placed teams
available after the competition closes).

<p>
<b>Team</b> 2 to 4 person team</b>
<p>
<b>Difficulty</b> depends on the topic, but often 4/5 or 5/5.

<hr>

<!----------------------------------------------------------------------------->

<p>
<b>Topic 1.2: Coda Lab datasets</b>
<p>
<b>Task description</b><br>
<a
href="https://github.com/codalab/codalab-worksheets/wiki/Competitions">Coda
Lab</a> hosts datasets, code and papers on some areas that may be of
interest, including Question Answering and Image Processing.

Question Answering is a task in the area of Natural Language Processing
that requires mapping a natural language question along with some knowledge
source (text or a structured database) into an answer. 
A number of Question Answering datasets are 
<a href="http://worksheets.codalab.org/worksheets/0x257ac973431d4c9282a75a64395be805/">here</a>.

This includes a live competition using the 
Stanford Question Answering Dataset (SQuAD), which can be accessed 
<a href="http://rajpurkar.github.io/SQuAD-explorer/">here</a>.

The Coda Lab worksheet on
<a href="https://worksheets.codalab.org/worksheets/0xeab681957fba4de6b6b0f0fa0d0b870b/">Image Classification</a> contains some popular image
classification datasets, such as ImageNet.

<p>
<b>Team</b> 2 to 4 person team</b>
<p>
<b>Difficulty</b> 5/5.

<hr>

<!----------------------------------------------------------------------------->

<p>
<b>Topic 1.3: Deep Reinforcement Learning / Transfer Learning</b>
<p>
<b>Task description</b><br>
There are a number of tasks, from introductory up to pretty much
state-of-the-art in reinforcement learning hosted at the OpenAI "Gym"
<a href="http://contest.openai.com/">site</a>.

<p>
However, <i>it is recommended that you do not attempt this unless you have
experience in both deep learning and reinforcement learning.</i>

<p>
There are a number of options, from basic control tasks to learning to
play Atari games to robot control tasks.
"Toy text" is introductory and is not sufficient by itself for this
assignment.

<p>
<b>Team</b> 2 to 4 person team</b>
<p>
<b>Difficulty</b> 3/5 for "Classic control", 4/5 for "Box2D" and
"Roboschool", and 5/5 for the other tasks.

<hr>

<!----------------------------------------------------------------------------->

<p>
<b>Topic 1.4: Learning for Artificial General Intelligence</b>
<p>
<b>Task description</b><br>
Microsoft researchers have made available environments for agents based on
Minecraft called  <a href="http://github.com/Microsoft/malmo">Project Malmo</a>. 
Can you select a task for agents in this environment and define a machine
learning problem to enable the agents to improve with experience on this
task ?

<p>
<b>Team</b> 2 to 4 person team</b>
<p>
<b>Difficulty</b> 5/5.

<hr>

<!----------------------------------------------------------------------------->

<p>
<b>Topic 1.5: Learning to predict "stance" of news articles</b>
<p>
<b>Task description</b><br>
The problem of automatically detecting "fake news", that is,
news stories that are made up with an intention to deceive, typically for
the purpose of some secondary gain, often financial, is known to be
difficult.
Another way of thinking about this problem is that it is an automated
version of <i>fact checking</i>, i.e., establishing the truth or otherwise
of some assertion.
Not surprisingly, in general this is a hard problem problem for AI.

<p> 
A <a href="http://www.fakenewschallenge.org/">contest</a> helpd in 2017 had
the aim of using machine learning to tackle an early stage of this
problem, that of predicting the "stance" of the text of a news article
relative to a headline.
In this setting the text may be said to agree, disagree, discuss or be
unrelated to the headline.

<p>
Although the contest is now over, the publicly available <a
href="https://github.com/FakeNewsChallenge/fnc-1">datasets</a> can be
used as a text classification problem.
You should follow the competition methodology and scoring system (a Python
script is provided for this purpose) as far as possible.
Note: the supplied <i>test</i> datasets are <b>NOT</b> to be used for
tuning or tweaking your algorithms, but only for the final evaluation of
whatever methods you use!

<p>
<b>Team</b> 2 to 4 person team</b>
<p>
<b>Difficulty</b> 5/5.

<hr>

<!----------------------------------------------------------------------------->

<p>
<b>Topic 1.6: Data repositories</b>
<p>
<b>Task description</b><br>
As an alternative to following the rules of past competitions you can
choose to apply machine learning to the datasets in new and interesting
ways, or you could do the same with datasets from some of the many
repositories now on the Web. The original such repository is the 
UC Irvine Machine Learning Repository
<a href="http://archive.ics.uci.edu/ml/index.html">here</a>.
These datasets are usually formatted for easy import into machine learning
software.
The principal usage of these datasets is to use as many as possible in a
<i>common task framework</i> to compare different algorithms, or different
parameters for the same algorithm.

<p>
MIT's Reality Commons
<a href="http://realitycommons.media.mit.edu/">project</a>
has some datasets on human behaviour obtained from
mobile phones and other devices, such as the one on
Human Activity Recognition Using Smartphones
<a href="http://realitycommons.media.mit.edu/realitymining.html">here</a>.

<p>
The U.S. Government's open data initiative
<a href="https://explore.data.gov/">Data.gov</a> contains many data sets.
The problem is that many are simply raw data and individual data sets do
not represent a well-defined learning problem; however, the hope is that
by linking several data sets some interesting mining may be possible.
Other governments doing similar things are in the
<a href="http://data.gov.uk/">U.K.</a>,
<a href="https://data.govt.nz/">N.Z</a> and 
<a href="http://data.gov.au/">Australia</a>. A more complete list is available
<a href="http://en.wikipedia.org/wiki/Open_data">here</a>.

<p>
Many of the above, plus more, are included in the list of repositories at
<a href="http://www.kdnuggets.com/datasets/index.html">KDnuggets</a>.

<p>
Pick a data set, review its publication history (if any) and see if you
can think of an interesting angle to explore to make a project.
The basic conditions are
<ul>
<li> assess carefully the time you will need to specify a well-posed
	problem and set up the dataset(s) and algorithm(s)
<li> give particular attention to defining a target function and ways in
	which you can evaluate the learning
<li> you must send the course admins an email with a description of what
	you are planning (a couple of paragraphs would usually be enough)
	that needs to be approved before you start
</ul>

<p>
<b>Team</b> 2 to 4 person team</b>
<p>
<b>Difficulty</b> assigned per topic (often 5/5).

<!----------------------------------------------------------------------------->

<hr>
<h3 align=center>2: Core methods</h3>
The objective of these topics is to give you an opportunity to get a
deeper understanding of some of the ideas we have covered in the course. 
Note: these are <b><i>"implement from scratch"</i></b> projects, that is,
you must not use any machine learning libraries, apart from utilities to
read data from files. This means that, for example, if you are
implementing in Python, you <b><i></i></b> cannot use any code from
scikit-learn (except for reading data from a file), although you can use
NumPy or SciPy code to implement your algorithm.
<hr>

<!----------------------------------------------------------------------------->

<p>
<b>Topic 2.1: Perceptron / Linear Unit</b>
<p>
<b>Task description</b><br>
Write a program to implement learning for numeric prediction / classification
based on an <i>n</i>-input linear unit / perceptron architecture.

First, implement the linear unit architecture
and the gradient descent learning method.
Test your system on the standard UCI data set
<a href="./data/autoMpg.arff"> autoMpg</a>.
Note that you will have to think about the representation
of input data for this task: some of the attributes
are real-valued, but others have discrete, ordered values.
One possibility is to use a Boolean variable (an "indicator" variable)
for each discrete value for such attributes, where the value is
1 if the instance has that value for the attribute, and 0 otherwise.

<p>
Second, implement the perceptron training algorithm to
learn <i>n</i>-input perceptron classifiers for Boolean functions.
Test on examples from at least two different <b>8-input</b> Boolean functions
(note: should include linearly-separable and non-linearly-separable functions).

<p>
Third, implement the "Pocket Algorithm" which is a straightforward
extension adding a kind of "weight memory" to the perceptron training
algorithm which can improve learning performance. Pseudo-code for the
algorithm is <a href="./pocket_algo.pdf"> here</a>.
Compare its learning performance with that of the perceptron training
algorithm (for example, by plotting the values of the pocket's accuracy
variables compared to those of the perceptron's during learning on
training data with different levels of added noise).

<p>
There is an option in this project to add graphical output showing
how the system is learning. This may require adding an extra person to
the team. The graphics do not have to be very
complicated, but should show key features of the learning algorithms,
such as weight update, or changes to the variables in the pocket
algorithm.

<p>
<b>Team</b> 2 or 3 person team<p>
<b>Difficulty</b> 3/5

<hr>

<!----------------------------------------------------------------------------->

<p>
<b>Topic 2.2: Nearest Neighbour</b>
<p>
<b>Task description</b><br>
Implement <i>k</i>-Nearest Neighbour (kNN) for both classification
and numeric prediction using both Manhattan (L1) and Euclidean (L2)
distances.

<p>
For classification, test your version of kNN for range of values of k
on the standard UCI data set <a href="./data/ionosphere.arff"> ionosphere</a>.
Evaluate your system by leave-one-out cross-validation.

<p>
For numeric prediction, test your version of kNN for range of values of k
on the standard UCI data set <a href="./data/autos.arff"> autos</a>.
For this data set you can remove examples with missing attribute values,
which will reduce the number of examples to 159.
The task is to predict the price given the 14 continuous and 1
integer attributes.
You can also experiment with encoding the other attributes to
allow their use in the distance function to see if this affects
predictive accuracy.
Evaluate your system by leave-one-out cross-validation.

<p>
Next you will extend your methods to 
implement distance-weighted Nearest Neighbour (WNN).
Rerun the algorithm for both the classification and numeric prediction
tasks.
Compare the results to analyse what difference (if any) distance-weighting
has made to the predictive performance.

<p>
Finally you must construct a probabilistic two-class classification <i>target
function</i> for which you know the class probabilities.
Calculate the Bayes error rate (refer to the "Classification"
lectures to review this if you are not sure).
Next, generate a dataset, with about the same number of features and
examples as the UCI datasets you used earlier, using this target function.
Now run your implementation of kNN and WNN on this dataset.
Experiment with the parameter settings for this algorithm and determine
how close to the Bayes error rate is the predictive performance.
Is this what you would have expected ? Justify your answer.

<p>
<b>Team</b> 2 or 3 person team<p>
<b>Difficulty</b> 3/5

<hr>

<!----------------------------------------------------------------------------->

<h3 align=center>3: Legacy methods</h3>

<p>
The objective of these topics is to broaden coverage by including options
that are no longer included in the course material, but may be of interest
for further study.
For each topic, versions of past lecture materials are provided as background.
<hr>

<!----------------------------------------------------------------------------->

<p>
<b>Topic 3.1: Genetic Algorithm</b>
<p>
<b>Task description</b><br>
Implement the basic Genetic Algorithm method as described
<a href="./ga.pdf"> here</a>, including the point mutation
rule and at least two of these crossover rules - single-point, two-point and
uniform. Test on the standard UCI data sets
<a href="./data/balance-scale.arff">
balance-scale</a>
and
<a href="./data/mushroom.arff">
mushroom</a>.

<p>
You will need to consider carefully the representations you use for the
classifiers to be learned for both data sets.
Also note that the mushroom data set is quite large.

<p>
There is an option in this project to add graphical output showing
how the system is learning. This requires adding an extra person to
the team (from 2 to 3 people). The graphics does not have to be very
complicated, but needs to show key features of the algorithm,
for example, the selection process to construct a new generation
of hypotheses.

<p>
<b>Team</b> 2 or 3 person team<p>
<b>Difficulty</b> 3/5

<hr>

<!----------------------------------------------------------------------------->

<p>
<b>Topic 3.2: Rule Learning by Sequential Covering</b>
<p>
<b>Task description</b><br>
Implement a Sequential Covering rule learning algorithm based on the
method described on slides 21-25 of these 
<a href="./ClassRules.pdf">notes</a>.
Your program should use a version of <i>information gain</i> from
decision tree learning to implement the "Performance()" measure to select
the attribute-value test to specialize the rule's condition part.

<p>
Test on these versions of the UCI data sets 
<a href="./data/mushroom.arff">
mushroom</a>
and
<a href="./data/splice2class.arff">
splice</a>.

<p>
Note that you do not need the instance name (first attribute) in the
splice data set, and that the mushroom data set is quite large !

<p>
There is an option in this project to add graphical output showing
how the system is learning. This requires adding an extra person to
the team (from 1 to 2  people). The graphics does not have to be very
complicated, but needs to show key features of the algorithm,
for example, the coverage and accuracy of the process of learning
a rule.

<p>
<b>Team</b> 2 person team<p>
<b>Difficulty</b> 3/5

<hr>

<!----------------------------------------------------------------------------->

<p>
<b>Topic 3.3: Inductive Logic Programming</b>
<p>
<b>Task description</b><br>
Implement an algorithm to compute the Relative Least General Generalization
(RLGG) of pairs of ground instances of the target predicate, as outlined
on slides 27-36 in these <a href="./ILP.pdf">lecture notes</a>.


<p>
Data set: In a typical 
<a href="http://en.wikipedia.org/wiki/Bongard_problem">Bongard problem</a>
there are 6 positive and 6 negative examples
of the hidden rule that correctly classifies a scene.
Select a small number (at least 2) of the Bongard problems from 
<a href="http://www.foundalis.com/res/bps/bpidx.htm">here</a>
(you could also make your own, or contact me if you need more information).

<p>
You will need to represent the scenes in such a way that the RLGG can be
represented as a single clause.
The RLGG is constructed from a pair of positive examples, so in a
typical Bongard problem there would be 15 possible pairs to choose
to initialize RLGG construction.

<p>
Once constructed, the RLGG clause (which is unique for a pair of positive
examples and fixed set of background clauses) can be further generalized
to ensure it covers all the positive examples and none of the negative
examples by dropping literals or turning constants to variables.

<p>
Represent the examples (pos and neg) and the background knowledge
in Prolog.
It may be easiest to implement the algorithm in Prolog too.

There are several high-quality open-source Prologs.
<a href="http://www.swi-prolog.org">SWI Prolog</a>
has the best documentation, with built-in help, whereas in general
<a href="http://www.dcc.fc.up.pt/~vsc/yap/">YAP Prolog</a>
enables faster running compiled programs.

<p>
<b>Team</b> 2 person team<p>
<b>Difficulty</b> 4/5

<hr>

<!----------------------------------------------------------------------------->

<p>
<b>Topic 3.4: Recommender system using collaborative filtering</b>
<p>
<b>Task description</b><br>
Pick <b>one</b> of the the following problems for recommendation and apply
collaborative filtering as described in these
<a href="./RecSys.pdf">lecture notes</a> (or an alternative). 
The data is from a collection collected by the GroupLens research
<a href="https://grouplens.org/datasets/movielens/"> group<a>.
Note that there are different versions of this data, with different
permissions.

<p> 
The suggested problems are either: learning to predict the ratings of books
on the 
<a href="http://www.grouplens.org/node/74"> BookCrossing dataset</a>, or
movie ratings on the 
<a href="http://www.grouplens.org/node/73"> MovieLens Data Sets</a> (these
are of different sizes, so start with the 100K).
However, other options are also available from the GroupLens site.

<p>
If you want to use one of the larger versions of the MovieLens data, there
is one at <a href="https://www.kaggle.com/grouplens/movielens-20m-dataset">Kaggle<a>.

<p>
<b>Team</b> 2 to 4 person team<p>
<b>Difficulty</b> 5/5

<hr>

<!----------------------------------------------------------------------------->

<p>
<b>Topic 3.5: Face recognition using neural networks</b>
<p>
<b>Task description</b><br>
This is Mitchell's face recognition task using neural networks.
The task is explained
<a href="http://www-2.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/faces.html">
here</a>.
See also Chapter 4 in Mitchell's book "Machine Learning" which is in the
library.
We showed an example of this neural network in lectures, and this will
allow you to get experience of neural networks without the use of modern
software libraries which hide a lot of the details.

<p> Note that although the task description is clearly dated, the problem
remains an interesting one. You need to complete the second part of the
task as well (ignore the ``optional'' heading !). This is an open-ended
assignment, so you will need to consider how far you can take this this.

<p>
<b>Team</b> 2 or 3 person team<p>
<b>Difficulty</b> 3/5

<hr>

<!----------------------------------------------------------------------------->

<p>
<b>Topic 3.6: Reinforcement Learning</b>
<p>
<b>Task description</b><br>
Implement a traffic light simulator for a single intersection and a
reinforcement learning system to learn how to switch the traffic lights
to reduce the delay for vehicles at the intersection.
You can use Q-learning, as described in these
<a href="./Reinforcement.pdf">notes</a>, or an alternative.

<p>
The details are explained
<a href="./data/traffic.txt">
here</a>
and there is a short movie showing the traffic light controller in
operation
<a href="./data/videoclip.avi">
here</a>.

<p>
Implementing graphics will help considerably in debugging this project.

<p>
<b>Team</b> 3 or 4 person team</b>
<p>
<b>Difficulty</b> 4/5 to 5/5 depending on extensions.

<hr>

<!----------------------------------------------------------------------------->

<h3 align=center>4: Miscellaneous</h3>

<p>
These options do not fit anywhere else, but may be of interest.
<hr>

<!----------------------------------------------------------------------------->
<p>
<b>Topic 4.1: Machine Learning for Bioinformatics</b>
<p>
<b>Task description</b><br>
Since these applications depend on domain knowledge, a team should
include at least one student majoring BE Bioinformatics, and you will need
to contact LiC if you are interested.

<p>
<b>Team</b> 2 to 4 person team</b>
<p>
<b>Difficulty</b> assigned per topic (probably 5/5).

<hr>

<!----------------------------------------------------------------------------->

<p>
<b>Topic 4.2: Learning to play "20 Questions"</b>
<p>
<b>Task description</b><br>
The traditional game often called "20 Questions" has been implemented in
at least two online versions that learn from user interaction, 
<a href=http://www.20q.net>20Q</a> and
<a href=http://en.akinator.com>Akinator</a>.
It is not clear how the learning is implemented for either of these
systems, but there appears to be a
<a href=http://www.google.com/patents/US20060230008?dq=Artificial%20neural%20network%20guessing%20method%20and%20game>patent application</a>
on the technology behind 20Q by its author, Robin Burgener, which outlines a form of neural network.

In this project, which is very open-ended, the objective is to implement a simple version of the twenty questions game that can interact with users and learn which questions to ask, thereby (hopefully) improving with experience.
Some general background and information about radio and TV versions is available on
<a href=https://en.wikipedia.org/wiki/Twenty_Questions>Wikipedia</a>.
There are various pointers on the web to different implementations, for example, in
<a href=http://openbookproject.net/py4fun/animal/animal.html>Python</a>, or
suggestions about the way in which this kind of system could use
<a href=http://stats.stackexchange.com/questions/6074/akinator-com-and-naive-bayes-classifier>machine learning</a>, but these should be treated with caution.
This project is only for those with experience in the rapid development of
interactive systems who are looking for a challenging application of
machine learning in this context.

<p>
<b>Team</b> 2 to 4 person team</b>
<p>
<b>Difficulty</b> 5/5

<hr>

<!----------------------------------------------------------------------------->

<h2>Report and assessment criteria</h2>

<p>
Assignment <a href="./criteria.pdf">criteria</a>.

<p>
Brief <a href="./reportguide.html">guide</a> to writing the report.

<p>
A basic sample <a href="./reportsample.txt">outline</a> of the structure
of a report.  Also, here is a recent
<a href="./reportsample.pdf">example</a> of a report from an earlier
iteration of the course.

<hr>
<h2>Submission</h2>

<p>
Submission is via <tt>give</tt>. You need to submit the project files and your report.

<p>
The project files should be submitted as a compressed <tt>tar</tt> or
<tt>zip</tt> archive. If your submission exceeds the current limit (2MB)
contact the course admins to arrange an alternative method of submission
for the excess materials, such as a download link.

<p>
The report must be in PDF format.

<p>
Combinations accepted are (if you are running from the Linux command-line):

<table>
<tr> <td> <tt> $ give cs9417 assignment files.tgz report.pdf </tt> </td> </tr>
<tr> <td> <tt> $ give cs9417 assignment files.zip report.pdf </tt> </td> </tr>
</table>

<hr>

<h2>Deadline</h2>
<t><b>
Sunday August 11 23:59:59
</b></t>
(contact course admins if you need an extension).

<hr>

<table align=right>
<tr><td><font size=-1>Last modified
Tue Jun 25 13:49:59 AEST 2019
</font></td></tr>
</table>


</body>
</html>
